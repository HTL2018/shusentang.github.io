---
title: AdaBoost算法详解与实现
date: 2018-11-18 22:05:04
toc: true
mathjax: true
categories: 
- Machine Learning
tags:
- AdaBoost
- 集成学习(ensemble)
---

目前网上大多数博客只介绍了AdaBoost算法是什么，但是鲜有人介绍为什么Adaboost长这样，本文对此给出了详细的解释。
<!--more-->

# 1. 概述
## 1.1 集成学习
我们知道目前存在各种各样的机器学习算法，例如SVM、决策树、感知机等等。但是实际应用中，或者说在打比赛时，成绩较好的队伍几乎都用了集成学习(ensemble learning)的方法。集成学习的思想，简单来讲，就是“三个臭皮匠顶个诸葛亮”。集成学习通过结合多个学习器(例如同种算法但是参数不同，或者不同算法)，一般会获得比任意单个学习器都要好的性能，尤其是在这些学习器都是"弱学习器"的时候提升效果会很明显。
> 弱学习器指的是性能不太好的学习器，比如一个准确率略微超过50%的二分类器。

下面看看西瓜书对此做的一个简单理论分析。
考虑一个二分类问题 $y \in \{-1, +1\}$ 、真实函数 $f$ 以及奇数 $T$ 个犯错概率**相互独立**且均为 $\epsilon $ 的学习器 $h_i$ 。我们用简单的投票进行集成学习，即分类结果取半数以上的学习器的结果:
$$
H(x) = sign(\sum_{i=1}^T h_i(x)) \tag{1.1}
$$


## 1.2 Boosting


